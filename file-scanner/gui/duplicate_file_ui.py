import streamlit as st
import pandas as pd
import psycopg2
from global_config.logger_config import logger
from global_config.config import yaml_config_boxed


# æ•°æ®åº“è¿æ¥å‚æ•°
DB_CONFIG = {
    "host": yaml_config_boxed.gui.postgres.host,
    "port": 5432,
    "dbname": yaml_config_boxed.gui.postgres.dbname,
    "user": yaml_config_boxed.gui.postgres.user,
    "password": yaml_config_boxed.gui.postgres.password
}

# ä¸‰ä¸ª SQL æŸ¥è¯¢
SQL_DUPLICATE_FILES = """
SELECT * FROM file_inventory fi
WHERE fi.deleted = 0
  AND EXISTS (
      SELECT 1 FROM file_inventory f
      WHERE f.deleted = 0
        AND f.md5 = fi.md5
        AND f.path != fi.path
  )
ORDER BY size DESC;
"""

SQL_ALL_SUM_SIZE = """
SELECT SUM(size) AS all_sum_size
FROM file_inventory fi
WHERE fi.deleted = 0
  AND EXISTS (
      SELECT 1 FROM file_inventory f
      WHERE f.deleted = 0
        AND f.md5 = fi.md5
        AND f.path != fi.path
  );
"""

SQL_DISTINCTIVE_SUM_SIZE = """
SELECT SUM(size) AS distinctive_sum_size FROM (
  SELECT DISTINCT fi.md5, fi.size
  FROM file_inventory fi
  WHERE fi.deleted = 0
    AND EXISTS (
        SELECT 1 FROM file_inventory f
        WHERE f.deleted = 0
          AND f.md5 = fi.md5
          AND f.path != fi.path
    )
) t;
"""

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“‚ é‡å¤æ–‡ä»¶åˆ†ææŠ¥è¡¨")

# è¿æ¥æ•°æ®åº“å¹¶æŸ¥è¯¢
@st.cache_data(ttl=300)
def run_queries():
    conn = psycopg2.connect(**DB_CONFIG)
    df_duplicates = pd.read_sql(SQL_DUPLICATE_FILES, conn)
    sum_all = pd.read_sql(SQL_ALL_SUM_SIZE, conn)
    sum_distinct = pd.read_sql(SQL_DISTINCTIVE_SUM_SIZE, conn)
    conn.close()
    return df_duplicates, sum_all.iloc[0, 0], sum_distinct.iloc[0, 0]

# æ‰§è¡ŒæŸ¥è¯¢
with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
    df_duplicates, all_sum, distinct_sum = run_queries()

# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
st.subheader("ğŸ“Š é‡å¤æ–‡ä»¶å¤§å°ç»Ÿè®¡")
st.metric(label="æ‰€æœ‰é‡å¤æ–‡ä»¶å¤§å°æ€»å’Œ", value=f"{all_sum / (1024**3):.2f} GB")
st.metric(label="å»é‡åçš„å¤§å°æ€»å’Œ", value=f"{distinct_sum / (1024**3):.2f} GB")
st.metric(label="å¯èŠ‚çœç©ºé—´", value=f"{(all_sum - distinct_sum) / (1024**3):.2f} GB")

# å±•ç¤ºé‡å¤æ–‡ä»¶åˆ—è¡¨
# st.subheader("ğŸ“ é‡å¤æ–‡ä»¶åˆ—è¡¨")
# st.dataframe(df_duplicates, use_container_width=True)

# pd.set_option("styler.render.max_elements", 10_000_000)  # æˆ–æ›´å¤§

# é«˜äº®é‡å¤æ–‡ä»¶ï¼šåŒä¸€ md5 åˆ†ç»„ï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸ºæ­£å¸¸é¢œè‰²ï¼Œå…¶ä½™ä¸ºæµ…çº¢è‰²èƒŒæ™¯
def highlight_duplicates(df):
    styles = pd.DataFrame('', index=df.index, columns=df.columns)
    for md5, group in df.groupby('md5'):
        if len(group) > 1:
            for i, idx in enumerate(group.index):
                if i > 0:  # é™¤äº†ç¬¬ä¸€ä¸ªï¼Œå…¶ä½™è®¾ç½®æµ…çº¢è‰²èƒŒæ™¯
                    styles.loc[idx, :] = 'background-color: #ffe6e6'
    return styles

# é‡æ–°è®¾ç½®ç´¢å¼•é¡ºåºï¼ˆé˜²æ­¢æ··ä¹±ï¼‰
df_duplicates_sorted = df_duplicates.sort_values(by=['size', 'md5', 'path'],ascending=[False,True,True])

# st.subheader("ğŸ“ é‡å¤æ–‡ä»¶åˆ—è¡¨ï¼ˆç›¸åŒmd5æ–‡ä»¶é«˜äº®æ˜¾ç¤ºï¼‰")
# st.dataframe(
#     df_duplicates_sorted.style.apply(highlight_duplicates, axis=None),
#     use_container_width=True
# )

MAX_ROWS = 500
df_show = df_duplicates_sorted.head(MAX_ROWS)

st.subheader(f"ğŸ“ é‡å¤æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰ {MAX_ROWS} æ¡ï¼Œé«˜äº®æ˜¾ç¤ºï¼‰")
st.dataframe(
    df_show.style.apply(highlight_duplicates, axis=None),
    use_container_width=True
)

# å…è®¸ä¸‹è½½å®Œæ•´ç»“æœï¼ˆæœªåŠ é¢œè‰²ï¼‰
st.download_button(
    label="ğŸ“¥ ä¸‹è½½å®Œæ•´é‡å¤æ–‡ä»¶åˆ—è¡¨ CSV",
    data=df_duplicates_sorted.to_csv(index=False),
    file_name="duplicate_files.csv",
    mime="text/csv"
)
