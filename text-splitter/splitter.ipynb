{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk beautifulsoup4 sentence-transformers transformers scikit-learn html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def clean_html(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.extract()\n",
    "    return html2text.html2text(str(soup))\n",
    "\n",
    "\n",
    "def extract_sentences(text: str):\n",
    "    # 统一多段落空行\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text.strip())\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "\n",
    "def cluster_sentences(sentences, model_name='all-MiniLM-L6-v2', threshold=1.0):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(sentences)\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=threshold)\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "    segments = []\n",
    "    current = []\n",
    "    last_label = labels[0]\n",
    "    for sent, label in zip(sentences, labels):\n",
    "        if label != last_label:\n",
    "            segments.append(\" \".join(current))\n",
    "            current = []\n",
    "        current.append(sent)\n",
    "        last_label = label\n",
    "    segments.append(\" \".join(current))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def enforce_token_limit(segments, max_tokens=2048, tokenizer_name='mistralai/Mistral-7B-Instruct-v0.1'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    final_chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for segment in segments:\n",
    "        if len(tokenizer.encode(current_chunk + segment)) > max_tokens:\n",
    "            if current_chunk:\n",
    "                final_chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            if len(tokenizer.encode(segment)) > max_tokens:\n",
    "                # 分段过长，进一步切\n",
    "                sentences = sent_tokenize(segment)\n",
    "                for s in sentences:\n",
    "                    if len(tokenizer.encode(current_chunk + s)) > max_tokens:\n",
    "                        final_chunks.append(current_chunk.strip())\n",
    "                        current_chunk = s\n",
    "                    else:\n",
    "                        current_chunk += \" \" + s\n",
    "                continue\n",
    "        current_chunk += \"\\n\" + segment\n",
    "    if current_chunk:\n",
    "        final_chunks.append(current_chunk.strip())\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def semantic_paragraph_split(text, is_html=False, max_tokens=2048):\n",
    "    if is_html:\n",
    "        text = clean_html(text)\n",
    "\n",
    "    sentences = extract_sentences(text)\n",
    "    segments = cluster_sentences(sentences)\n",
    "    chunks = enforce_token_limit(segments, max_tokens=max_tokens)\n",
    "    return [{\"id\": i, \"content\": chunk} for i, chunk in enumerate(chunks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "import os\n",
    "path = \"sample.md\"\n",
    "path = os.path.expanduser(\"~/Downloads/示例html.html\")\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "results = semantic_paragraph_split(raw_text, is_html=False, max_tokens=2048)\n",
    "\n",
    "with open(\"split_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"段落拆分完成，共分得 {} 段。\".format(len(results)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
