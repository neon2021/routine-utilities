import os
import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import timedelta
import re

st.set_page_config(layout="wide")

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
MODELS = {
    "MiniLM": "all-MiniLM-L6-v2",
    # "MPNet-Multilingual": "paraphrase-multilingual-mpnet-base-v2",
    # "E5-Large-v2": "intfloat/e5-large-v2",
}

# ç¼“å­˜åŠ è½½å¤šä¸ªæ¨¡å‹
@st.cache_resource
def load_models():
    return {name: SentenceTransformer(path) for name, path in MODELS.items()}

models = load_models()

# å­—å¹•æ—¶é—´æ ¼å¼å¤„ç†
def time_to_timedelta(t):
    h, m, s_ms = t.split(":")
    s, ms = s_ms.split(",")
    return timedelta(hours=int(h), minutes=int(m), seconds=int(s), milliseconds=int(ms))

# è§£æ SRT æ–‡ä»¶
def parse_srt(srt_content):
    pattern = re.compile(r"(\d+)\s+(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\s+(.*?)\s+(?=\d+\s+\d{2}|\Z)", re.DOTALL)
    entries = []
    for match in pattern.finditer(srt_content):
        entries.append({
            "index": int(match.group(1)),
            "start": match.group(2),
            "end": match.group(3),
            "text": match.group(4).replace("\n", " ").strip()
        })
    return entries


import spacy

@st.cache_resource
def load_spacy():
    return spacy.load("en_core_web_sm")

nlp = load_spacy()

import re

# def should_merge(prev_text, curr_text, similarity, sim_threshold, gap_sec):
#     # rule 1: semantic similarity
#     if gap_sec < 1.5 and similarity > sim_threshold:
#         return True

#     prev_text = prev_text.strip()
#     curr_text = curr_text.strip()
#     if not prev_text or not curr_text:
#         return False

#     # rule 2: simple structure clues
#     prev_ends_punct = prev_text[-1] in ".!?,ï¼Œã€‚ï¼ï¼Ÿ"
#     curr_first_word = curr_text.split()[0].lower()

#     conj_starters = {'and', 'but', 'plus', 'also', 'then', 'so', 'or'}
#     verb_starters = {'to', 'deport', 'resume', 'support', 'give', 'take', 'stop'}

#     if not prev_ends_punct and (curr_first_word in conj_starters or curr_first_word in verb_starters):
#         return True

#     # rule 3: to + verb structure
#     if prev_text.endswith('to') and curr_first_word in verb_starters:
#         return True

#     # âœ… rule 4: SpaCy-based structure analysis
#     prev_doc = nlp(prev_text)
#     curr_doc = nlp(curr_text)

#     # å‰ä¸€å¥æ²¡æœ‰ä¸»è¯­æˆ–åŠ¨è¯ç»“å°¾ï¼Œå¯èƒ½æ˜¯æ®‹å¥
#     prev_incomplete = all(tok.dep_ not in {'ROOT'} for tok in prev_doc)

#     # å½“å‰å¥ä»¥åŠ¨è¯æˆ–è¿è¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯è¡¥è¯­
#     curr_starts_with_verb_or_conj = (
#         curr_doc[0].pos_ in {'VERB', 'AUX', 'CCONJ', 'PART', 'ADP'}  # åŠ¨è¯ã€åŠ©åŠ¨è¯ã€è¿è¯ã€ä»‹è¯
#     )

#     if prev_incomplete and curr_starts_with_verb_or_conj:
#         return True

#     return False

# def should_merge(prev_text, curr_text, similarity, sim_threshold, gap_sec):
#     nlp = load_spacy()  # cached load

#     if gap_sec < 1.5 and similarity > sim_threshold:
#         return True

#     prev_text = prev_text.strip()
#     curr_text = curr_text.strip()
#     if not prev_text or not curr_text:
#         return False

#     prev_doc = nlp(prev_text)
#     curr_doc = nlp(curr_text)

#     prev_ends_punct = prev_text[-1] in ".!?ã€‚ï¼ï¼Ÿ"
#     curr_first_token = curr_doc[0] if curr_doc else None
#     prev_last_token = prev_doc[-1] if prev_doc else None

#     # rule: è¡¥å¥è¿è¯ã€åŠ¨è¯å¼€å¤´ï¼ˆç®€å•ï¼‰
#     if not prev_ends_punct and curr_first_token and curr_first_token.text.lower() in {
#         'and', 'but', 'plus', 'also', 'then', 'so', 'to', 'deport'
#     }:
#         return True

#     # rule: to + verb
#     if prev_text.endswith('to') and curr_first_token and curr_first_token.pos_ == "VERB":
#         return True

#     # âœ… rule: å‰å¥ä»¥ä»‹è¯ç»“å°¾ï¼Œåå¥ä»¥åè¯å¼€å¤´
#     if prev_last_token and curr_first_token:
#         if prev_last_token.pos_ == "ADP" and curr_first_token.pos_ in {"NOUN", "PROPN", "DET"}:
#             return True

#     # âœ… rule: å‰å¥ä»¥å…³ç³»è¯ï¼ˆwhich, where, thatï¼‰ç»“å°¾ï¼Œåå¥æ˜¯ä»å¥è¡¥å……
#     if prev_last_token and prev_last_token.text.lower() in {"where", "which", "that"}:
#         return True

#     return False

def should_merge_old(prev_text, curr_text, similarity, sim_threshold, gap_sec):
    nlp = load_spacy()  # ä½¿ç”¨ Streamlit cache_resource

    # Rule 1: è¯­ä¹‰ç›¸ä¼¼åº¦ + æ—¶é—´é—´éš”
    if gap_sec < 1.5 and similarity > sim_threshold:
        return True

    prev_text = prev_text.strip()
    curr_text = curr_text.strip()
    if not prev_text or not curr_text:
        return False

    prev_doc = nlp(prev_text)
    curr_doc = nlp(curr_text)

    # è·å–æœ‰æ•ˆ tokenï¼ˆå»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    prev_tokens = [t for t in prev_doc if not t.is_space]
    curr_tokens = [t for t in curr_doc if not t.is_space]

    if not prev_tokens or not curr_tokens:
        return False

    prev_last = prev_tokens[-1]
    curr_first = curr_tokens[0]

    # Rule 2: å¸¸è§è¡¥å¥è¿æ¥è¯ï¼ˆand, but, to ç­‰ï¼‰
    if prev_text[-1] not in ".!?ï¼Œã€‚ï¼ï¼Ÿ" and curr_first.text.lower() in {
        'and', 'but', 'plus', 'also', 'then', 'so', 'to', 'deport', 'from'
    }:
        return True

    # Rule 3: å‰å¥ä»¥ "to" ç»“å°¾ + åå¥åŠ¨è¯
    if prev_last.text.lower() == 'to' and curr_first.pos_ in {"VERB", "AUX"}:
        return True

    # âœ… Rule 4: å‰å¥ä»¥ä»‹è¯ç»“å°¾ + åå¥ä»¥åè¯/ä¸“æœ‰åè¯/é™å®šè¯å¼€å¤´
    if prev_last.pos_ == "ADP" and curr_first.pos_ in {"NOUN", "PROPN", "DET"}:
        return True

    # âœ… Rule 5: å‰å¥ä»¥å…³ç³»è¯ç»“å°¾ï¼ˆwhich, where, thatï¼‰+ åå¥ä»å¥ç»“æ„
    if prev_last.text.lower() in {"where", "which", "that"}:
        return True

    # âœ… Rule 6: å‰å¥ä»¥ "back where" æˆ– "from the" ç­‰å¸¸è§ç»“æ„ç»“å°¾
    prev_phrase = prev_text.strip().lower()
    if prev_phrase.endswith("from the") or prev_phrase.endswith("back where"):
        return True
    
    # âœ… Rule 7: å‰å¥ä»¥ how/when/why ç­‰å¼•å¯¼è¯ç»“å°¾ï¼Œåå¥ä»¥å‰¯è¯/ä»‹è¯çŸ­è¯­å¼€å¤´
    if prev_last.text.lower() in {"how", "when", "why"} and curr_first.pos_ in {"ADP", "ADV"}:
        return True

    return False

import requests

def query_ollama_merge_decision(prev, curr, model="gemma:7b"):
    prompt = f"""è¯·åˆ¤æ–­ä¸‹é¢ä¸¤ä¸ªå­—å¹•æ˜¯å¦å±äºåŒä¸€ä¸ªå®Œæ•´å¥å­ï¼ˆå³å®ƒä»¬æ„æˆä¸€ä¸ªå®Œæ•´çš„è¯­æ³•å•å…ƒï¼Œä¸åº”æ–­å¼€ï¼‰ã€‚

å­—å¹•1ï¼š
{prev}

å­—å¹•2ï¼š
{curr}

æ˜¯å¦åº”åˆå¹¶ï¼Ÿåªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚"""
    
    response = requests.post("http://localhost:11434/api/generate", json={
        "model": model,
        "prompt": prompt,
        "stream": False,
    })
    print(f'model: {model}; prompt:{prompt}')
    reply = response.json()["response"].strip()
    print(f'reply:{reply}')
    return reply.startswith("æ˜¯")


def should_merge(prev_text, curr_text, similarity, sim_threshold, gap_sec):
    # ä¼˜å…ˆè°ƒç”¨ Ollama + gemma è¿›è¡Œåˆ¤æ–­
    try:
        if query_ollama_merge_decision(prev_text, curr_text, model="gemma3:4b"):
            # print(f"âœ… åˆå¹¶ï¼ˆgemmaåˆ¤æ–­ï¼‰: \n  prev: {prev_text}\n  curr: {curr_text}")
            return True
    except Exception as e:
        print(f"âš ï¸ Ollama è¯·æ±‚å¤±è´¥ï¼Œé™çº§ä½¿ç”¨ç›¸ä¼¼åº¦åˆ¤æ–­ï¼š{e}")

    # fallback
    return should_merge_old(prev_text, curr_text, similarity, sim_threshold, gap_sec)


# åˆå¹¶é€»è¾‘ï¼ˆä¼ å…¥æ¨¡å‹ï¼‰
def merge_entries(entries, model, time_gap_threshold=1.5, sim_threshold=0.75):
    merged = []
    buffer = [entries[0]]
    for curr in entries[1:]:
        prev = buffer[-1]
        prev_end = time_to_timedelta(prev["end"])
        curr_start = time_to_timedelta(curr["start"])
        gap = (curr_start - prev_end).total_seconds()

        emb1 = model.encode([prev["text"]])[0]
        emb2 = model.encode([curr["text"]])[0]
        similarity = cosine_similarity([emb1], [emb2])[0][0]

        if should_merge(prev["text"], curr["text"], similarity, sim_threshold, gap):
            buffer.append(curr)
        else:
            merged.append({
                "start": buffer[0]["start"],
                "end": buffer[-1]["end"],
                "text": " ".join([b["text"] for b in buffer])
            })
            buffer = [curr]
    if buffer:
        merged.append({
            "start": buffer[0]["start"],
            "end": buffer[-1]["end"],
            "text": " ".join([b["text"] for b in buffer])
        })
    for i, m in enumerate(merged, 1):
        m["index"] = i
    return merged

# åˆå¹¶å¹¶ç¼“å­˜æ¯ä¸ªæ¨¡å‹ç»“æœ
@st.cache_data(show_spinner="ğŸ§  æ¨¡å‹åˆå¹¶å¤„ç†ä¸­...")
def get_all_model_outputs(srt_text, sim_threshold, gap_threshold):
    entries = parse_srt(srt_text)
    results = {}
    for name, model in models.items():
        merged = merge_entries(entries, model, gap_threshold, sim_threshold)
        results[name] = merged
    return entries, results

# SRT æ–‡ä»¶è¯»å–
srt_path = os.path.join(os.path.dirname(__file__), "example.srt")
with open(srt_path, 'r') as f:
    srt_text = f.read()

# æ§ä»¶åŒº
st.sidebar.title("ğŸ”§ è®¾ç½®å‚æ•°")
sim_threshold = st.sidebar.slider("è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼", 0.5, 0.95, 0.75, 0.01)
gap_threshold = st.sidebar.slider("æœ€å¤§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰", 0.1, 5.0, 1.5, 0.1)

entries, all_results = get_all_model_outputs(srt_text, sim_threshold, gap_threshold)

# å¹¶æ’æ˜¾ç¤ºåŸæ–‡ + å¤šæ¨¡å‹ç»“æœ
cols = st.columns(len(models) + 1)

with cols[0]:
    st.markdown("### ğŸ“œ åŸå§‹å­—å¹•")
    for e in entries[:20]:
        st.text(f"[{e['start']} â†’ {e['end']}] {e['text']}")

for i, (name, merged) in enumerate(all_results.items(), start=1):
    with cols[i]:
        st.markdown(f"### ğŸ¤– {name}")
        for e in merged[:10]:
            st.markdown(f"**[{e['start']} â†’ {e['end']}]** {e['text']}")
