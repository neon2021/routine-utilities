import os
import streamlit as st

from v03_srt_merger_llm_func import *

st.set_page_config(layout="wide")


# def should_merge(prev_text, curr_text, similarity, sim_threshold, gap_sec):
#     # rule 1: semantic similarity
#     if gap_sec < 1.5 and similarity > sim_threshold:
#         return True

#     prev_text = prev_text.strip()
#     curr_text = curr_text.strip()
#     if not prev_text or not curr_text:
#         return False

#     # rule 2: simple structure clues
#     prev_ends_punct = prev_text[-1] in ".!?,ï¼Œã€‚ï¼ï¼Ÿ"
#     curr_first_word = curr_text.split()[0].lower()

#     conj_starters = {'and', 'but', 'plus', 'also', 'then', 'so', 'or'}
#     verb_starters = {'to', 'deport', 'resume', 'support', 'give', 'take', 'stop'}

#     if not prev_ends_punct and (curr_first_word in conj_starters or curr_first_word in verb_starters):
#         return True

#     # rule 3: to + verb structure
#     if prev_text.endswith('to') and curr_first_word in verb_starters:
#         return True

#     # âœ… rule 4: SpaCy-based structure analysis
#     prev_doc = nlp(prev_text)
#     curr_doc = nlp(curr_text)

#     # å‰ä¸€å¥æ²¡æœ‰ä¸»è¯­æˆ–åŠ¨è¯ç»“å°¾ï¼Œå¯èƒ½æ˜¯æ®‹å¥
#     prev_incomplete = all(tok.dep_ not in {'ROOT'} for tok in prev_doc)

#     # å½“å‰å¥ä»¥åŠ¨è¯æˆ–è¿è¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯è¡¥è¯­
#     curr_starts_with_verb_or_conj = (
#         curr_doc[0].pos_ in {'VERB', 'AUX', 'CCONJ', 'PART', 'ADP'}  # åŠ¨è¯ã€åŠ©åŠ¨è¯ã€è¿è¯ã€ä»‹è¯
#     )

#     if prev_incomplete and curr_starts_with_verb_or_conj:
#         return True

#     return False

# def should_merge(prev_text, curr_text, similarity, sim_threshold, gap_sec):
#     nlp = load_spacy()  # cached load

#     if gap_sec < 1.5 and similarity > sim_threshold:
#         return True

#     prev_text = prev_text.strip()
#     curr_text = curr_text.strip()
#     if not prev_text or not curr_text:
#         return False

#     prev_doc = nlp(prev_text)
#     curr_doc = nlp(curr_text)

#     prev_ends_punct = prev_text[-1] in ".!?ã€‚ï¼ï¼Ÿ"
#     curr_first_token = curr_doc[0] if curr_doc else None
#     prev_last_token = prev_doc[-1] if prev_doc else None

#     # rule: è¡¥å¥è¿è¯ã€åŠ¨è¯å¼€å¤´ï¼ˆç®€å•ï¼‰
#     if not prev_ends_punct and curr_first_token and curr_first_token.text.lower() in {
#         'and', 'but', 'plus', 'also', 'then', 'so', 'to', 'deport'
#     }:
#         return True

#     # rule: to + verb
#     if prev_text.endswith('to') and curr_first_token and curr_first_token.pos_ == "VERB":
#         return True

#     # âœ… rule: å‰å¥ä»¥ä»‹è¯ç»“å°¾ï¼Œåå¥ä»¥åè¯å¼€å¤´
#     if prev_last_token and curr_first_token:
#         if prev_last_token.pos_ == "ADP" and curr_first_token.pos_ in {"NOUN", "PROPN", "DET"}:
#             return True

#     # âœ… rule: å‰å¥ä»¥å…³ç³»è¯ï¼ˆwhich, where, thatï¼‰ç»“å°¾ï¼Œåå¥æ˜¯ä»å¥è¡¥å……
#     if prev_last_token and prev_last_token.text.lower() in {"where", "which", "that"}:
#         return True

#     return False


# SRT æ–‡ä»¶è¯»å–
srt_path = os.path.join(os.path.dirname(__file__), "example.srt")
with open(srt_path, 'r') as f:
    srt_text = f.read()

# æ§ä»¶åŒº
st.sidebar.title("ğŸ”§ è®¾ç½®å‚æ•°")
sim_threshold = st.sidebar.slider("è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼", 0.5, 0.95, 0.75, 0.01)
gap_threshold = st.sidebar.slider("æœ€å¤§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰", 0.1, 5.0, 1.5, 0.1)

entries, all_results = get_all_model_outputs(srt_text, sim_threshold, gap_threshold)

# å¹¶æ’æ˜¾ç¤ºåŸæ–‡ + å¤šæ¨¡å‹ç»“æœ
cols = st.columns(len(models) + 1)

with cols[0]:
    st.markdown("### ğŸ“œ åŸå§‹å­—å¹•")
    for e in entries[:20]:
        st.text(f"[{e['start']} â†’ {e['end']}] {e['text']}")

for i, (name, merged) in enumerate(all_results.items(), start=1):
    with cols[i]:
        st.markdown(f"### ğŸ¤– {name}")
        for e in merged[:10]:
            st.markdown(f"**[{e['start']} â†’ {e['end']}]** {e['text']}")
